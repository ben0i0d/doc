前言

算了，发哔哩哔哩了，懒得搭博客或者去发文章了，写点顺便分享给一些找我要的人。这份架构并不是我们后续要使用的，下个周期我们会重新构建IAAS。
目标

本文主要介绍我实验室当前使用的云服务集群，当前架构版号V24.1.28_1,以下架构均已通过生产环境验证并稳定运行至少6个月。

我们希望集群能够承担以下负载能力
1.数据可靠，具有去单点故障能力、热备能力

2.弹性化扩缩容

3.完成包括GPU调度等问题
架构介绍

以下我按照IAAS，PAAS，SAAS层次介绍
IAAS层次

虚拟化方案：proxmox ve（KVM）

我们建议对于刚上手虚拟化的人，选择成熟可靠好上手的架构，不要选择openstack这样的大架构，根本玩不起来

我并不想介绍为什么不推荐ESXI，那样会带来争议，我选择PVE单纯因为开源免费公开透明。

当然PVE不是完美的，我会在下一个周期重构完服务器再讨论这一点。

数据方案：

本地存储：ZFS

关于zfs改内存，我直接给512MB，因为ZFS内存实际上没有那么重要，感觉ZFS后面NAS用的不多估计也是宣传“1T数据1G ecc内存”把人吓到了

然后我再分享一个冷知识,一定要会用vdev，尤其是进行容量更换，raid等级更换，用好vdev非常方便，我们曾经把根系统变到64GU盘，又折腾到硬盘，然后又从500G raid1 换到300G raid1。完全不需要重装，非常舒服。



共享存储：CEPH

如果你真的准备用ceph，没有万兆网络不用考虑了，网络怎么架构我下面说

首先是安装，PVE现在装ceph比较方便，因为放在自己的仓库了，但是如果你是PVE<=7,你就得用反向代理了，因为默认是从download.proxmox下，你反向代理到中科大的Proxmox源。（你搜一下，然后用nginx就行非常简单）

然后是使用，真要用ceph就必须把ceph和PVE脱钩，要不然ceph的对象存储用不来，而且不好深度操作。怎么操作呢？看这个 https://docs.ceph.com/en/latest/cephadm/adoption/

cephadm使用podman，镜像放在quay.io。你要记得给podman配置好代理，要不然拉不下来镜像。

脱钩完了以后，重点来了。ceph几个重点问题
1.DB与WAL配置

要说这个，先看一个IO图

read-normal-img

客户端向主PG写，主PG向副本写，只要WAL写结束，写入就会ACK。

所以这就是为什么“DB要比block区域快，WAL区域比DB快”，日志设备快，写入完成就快，写IO延迟就小。但是实际上你不单独配置WAL也OK，它会使用DB区域

DB大小，RBD需求就是block区域大小的1%-2%，对象存储RGW，就要4%。

我建议配置DB，因为我们之前写入延迟10%，配置以后，稳定降低到0.5%。

2.设备分类

我很喜欢CEPH的一个点就在于配合device class和crush rule可以让数据选择性落入我希望的设备中。

因为现实中包括我们实验室存储设备包括4t 7.2k hdd，300g 10k hdd，nvme等设备。

我们肯定希望对象存储扔到大容量机械盘，rbd放10k的高速盘，元数据放nvme。(没钱全闪，看我们标题都是低成本啦)

所以简单方案就是，新建一个class叫hddl，然后新建crush rule，把rgw.data改成这个rule就行。

3.怎么将pve和cephadm配合使用

pve上的管理都能用，就是看不到日志罢了，cephadm实际上主要也就是为了搞定rgw。

4.故障域

建议放host就行

网络方案

1.路由器



